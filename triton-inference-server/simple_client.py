#!/usr/bin/env python3
"""
ç®€å•çš„ Triton å®¢æˆ·ç«¯ç¤ºä¾‹
"""

import numpy as np
import tritonclient.http as httpclient


def simple_inference():
    """ç®€å•çš„æ¨ç†ç¤ºä¾‹"""
    
    # è¿æ¥åˆ° Triton æœåŠ¡å™¨
    url = "localhost:8000"
    client = httpclient.InferenceServerClient(url=url)
    
    # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
    if not client.is_server_live():
        print("âŒ Triton æœåŠ¡å™¨æœªè¿è¡Œ")
        return
    
    print("âœ… Triton æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
    
    # æ¨¡å‹åç§°
    model_name = "Times_Classify"
    
    # å‡†å¤‡è¾“å…¥æ•°æ® (20x14 çš„æ—¶é—´åºåˆ—æ•°æ®)
    input_data = np.random.randn(1, 20, 14).astype(np.float32)  # æ‰¹æ¬¡å¤§å°=1
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")
    
    # åˆ›å»ºè¾“å…¥å¯¹è±¡
    inputs = [httpclient.InferInput("input", input_data.shape, "FP32")]
    inputs[0].set_data_from_numpy(input_data)
    
    # åˆ›å»ºè¾“å‡ºå¯¹è±¡
    outputs = [httpclient.InferRequestedOutput("output")]
    
    try:
        # æ‰§è¡Œæ¨ç†
        results = client.infer(
            model_name=model_name,
            inputs=inputs,
            outputs=outputs
        )
        
        # è·å–è¾“å‡º
        output = results.as_numpy("output")
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"åŸå§‹è¾“å‡º: {output[0]}")
        
        # è®¡ç®— softmax æ¦‚ç‡
        exp_output = np.exp(output[0] - np.max(output[0]))
        probabilities = exp_output / np.sum(exp_output)
        
        # è·å–é¢„æµ‹ç»“æœ
        predicted_class = np.argmax(probabilities)
        confidence = probabilities[predicted_class]
        
        # ç±»åˆ«æ ‡ç­¾
        labels = ["bird", "uav"]
        predicted_label = labels[predicted_class]
        
        print(f"\nğŸ¯ é¢„æµ‹ç»“æœ:")
        print(f"é¢„æµ‹ç±»åˆ«: {predicted_label}")
        print(f"ç½®ä¿¡åº¦: {confidence:.4f}")
        print(f"æ¦‚ç‡åˆ†å¸ƒ: bird={probabilities[0]:.4f}, uav={probabilities[1]:.4f}")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")


if __name__ == "__main__":
    simple_inference()
