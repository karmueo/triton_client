#!/usr/bin/env python3
"""
Triton å®¢æˆ·ç«¯ç¤ºä¾‹ - è°ƒç”¨ Times_Classify æ¨¡å‹
"""

import numpy as np
import tritonclient.http as httpclient
import tritonclient.grpc as grpcclient
from tritonclient.utils import triton_to_np_dtype
import argparse
import json
import time


class TritonClient:
    def __init__(self, url="localhost:8000", protocol="http"):
        """
        åˆå§‹åŒ– Triton å®¢æˆ·ç«¯
        
        Args:
            url: Triton æœåŠ¡å™¨åœ°å€
            protocol: åè®®ç±»å‹ ("http" æˆ– "grpc")
        """
        self.url = url
        self.protocol = protocol
        
        if protocol == "http":
            self.client = httpclient.InferenceServerClient(url=url)
        elif protocol == "grpc":
            self.client = grpcclient.InferenceServerClient(url=url)
        else:
            raise ValueError("åè®®ç±»å‹å¿…é¡»æ˜¯ 'http' æˆ– 'grpc'")
    
    def check_server_health(self):
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            if self.client.is_server_live():
                print("âœ… Triton æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
                return True
            else:
                print("âŒ Triton æœåŠ¡å™¨æœªå“åº”")
                return False
        except Exception as e:
            print(f"âŒ è¿æ¥æœåŠ¡å™¨å¤±è´¥: {e}")
            return False
    
    def get_model_info(self, model_name):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        try:
            model_metadata = self.client.get_model_metadata(model_name)
            model_config = self.client.get_model_config(model_name)
            
            print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯: {model_name}")
            print(f"å¹³å°: {model_config['platform']}")
            print(f"æœ€å¤§æ‰¹å¤„ç†å¤§å°: {model_config['max_batch_size']}")
            
            print("\nè¾“å…¥å‚æ•°:")
            for input_info in model_metadata['inputs']:
                print(f"  - åç§°: {input_info['name']}")
                print(f"    æ•°æ®ç±»å‹: {input_info['datatype']}")
                print(f"    ç»´åº¦: {input_info['shape']}")
            
            print("\nè¾“å‡ºå‚æ•°:")
            for output_info in model_metadata['outputs']:
                print(f"  - åç§°: {output_info['name']}")
                print(f"    æ•°æ®ç±»å‹: {output_info['datatype']}")
                print(f"    ç»´åº¦: {output_info['shape']}")
            
            return model_metadata, model_config
        except Exception as e:
            print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return None, None
    
    def list_models(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        try:
            models = self.client.get_model_repository_index()
            print("\nğŸ“¦ å¯ç”¨æ¨¡å‹:")
            for model in models:
                print(f"  - {model['name']} (çŠ¶æ€: {model['state']})")
            return models
        except Exception as e:
            print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def prepare_input_data(self, data, input_name="input", batch_size=1):
        """
        å‡†å¤‡è¾“å…¥æ•°æ®
        
        Args:
            data: è¾“å…¥æ•°æ® (numpy array æˆ– list)
            input_name: è¾“å…¥åç§°
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        if isinstance(data, list):
            data = np.array(data, dtype=np.float32)
        
        # ç¡®ä¿æ•°æ®æ˜¯ float32 ç±»å‹
        if data.dtype != np.float32:
            data = data.astype(np.float32)
        
        # å¦‚æœæ•°æ®æ²¡æœ‰æ‰¹å¤„ç†ç»´åº¦ï¼Œæ·»åŠ æ‰¹å¤„ç†ç»´åº¦
        if len(data.shape) == 2 and data.shape[0] == 20 and data.shape[1] == 14:
            data = np.expand_dims(data, axis=0)  # æ·»åŠ æ‰¹å¤„ç†ç»´åº¦
        
        # é‡å¤æ•°æ®ä»¥åŒ¹é…æ‰¹å¤„ç†å¤§å°
        if batch_size > 1:
            data = np.repeat(data, batch_size, axis=0)
        
        print(f"ğŸ“¥ è¾“å…¥æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"ğŸ“¥ è¾“å…¥æ•°æ®ç±»å‹: {data.dtype}")
        
        # åˆ›å»ºè¾“å…¥å¯¹è±¡
        if self.protocol == "http":
            inputs = [httpclient.InferInput(input_name, data.shape, "FP32")]
            inputs[0].set_data_from_numpy(data)
        else:
            inputs = [grpcclient.InferInput(input_name, data.shape, "FP32")]
            inputs[0].set_data_from_numpy(data)
        
        return inputs
    
    def prepare_outputs(self, output_names=["output"]):
        """å‡†å¤‡è¾“å‡ºé…ç½®"""
        if self.protocol == "http":
            outputs = [httpclient.InferRequestedOutput(name) for name in output_names]
        else:
            outputs = [grpcclient.InferRequestedOutput(name) for name in output_names]
        
        return outputs
    
    def infer(self, model_name, inputs, outputs):
        """æ‰§è¡Œæ¨ç†"""
        try:
            start_time = time.time()
            
            # æ‰§è¡Œæ¨ç†
            results = self.client.infer(
                model_name=model_name,
                inputs=inputs,
                outputs=outputs
            )
            
            inference_time = time.time() - start_time
            
            # è·å–è¾“å‡ºç»“æœ
            output_data = results.as_numpy('output')
            
            print(f"âš¡ æ¨ç†æ—¶é—´: {inference_time:.4f} ç§’")
            print(f"ğŸ“¤ è¾“å‡ºå½¢çŠ¶: {output_data.shape}")
            print(f"ğŸ“¤ è¾“å‡ºæ•°æ®ç±»å‹: {output_data.dtype}")
            
            return output_data, inference_time
            
        except Exception as e:
            print(f"âŒ æ¨ç†å¤±è´¥: {e}")
            return None, None
    
    def predict_with_labels(self, model_name, data, labels=["bird", "uav"]):
        """
        æ‰§è¡Œé¢„æµ‹å¹¶è¿”å›å¸¦æ ‡ç­¾çš„ç»“æœ
        
        Args:
            model_name: æ¨¡å‹åç§°
            data: è¾“å…¥æ•°æ®
            labels: ç±»åˆ«æ ‡ç­¾åˆ—è¡¨
        """
        # å‡†å¤‡è¾“å…¥æ•°æ®
        inputs = self.prepare_input_data(data)
        outputs = self.prepare_outputs()
        
        # æ‰§è¡Œæ¨ç†
        output_data, inference_time = self.infer(model_name, inputs, outputs)
        
        if output_data is not None:
            # åº”ç”¨ softmax è·å–æ¦‚ç‡
            probabilities = self.softmax(output_data[0])  # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„ç»“æœ
            
            # è·å–é¢„æµ‹ç±»åˆ«
            predicted_class = np.argmax(probabilities)
            predicted_label = labels[predicted_class] if predicted_class < len(labels) else f"Class_{predicted_class}"
            confidence = probabilities[predicted_class]
            
            print(f"\nğŸ¯ é¢„æµ‹ç»“æœ:")
            print(f"é¢„æµ‹ç±»åˆ«: {predicted_label}")
            print(f"ç½®ä¿¡åº¦: {confidence:.4f}")
            print(f"åŸå§‹è¾“å‡º: {output_data[0]}")
            print(f"æ¦‚ç‡åˆ†å¸ƒ: {probabilities}")
            
            return {
                "predicted_class": predicted_class,
                "predicted_label": predicted_label,
                "confidence": float(confidence),
                "probabilities": probabilities.tolist(),
                "raw_output": output_data[0].tolist(),
                "inference_time": inference_time
            }
        
        return None
    
    @staticmethod
    def softmax(x):
        """è®¡ç®— softmax"""
        exp_x = np.exp(x - np.max(x))  # å‡å»æœ€å¤§å€¼ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
        return exp_x / np.sum(exp_x)


def generate_sample_data():
    """ç”Ÿæˆç¤ºä¾‹æ•°æ® (20x14 çš„æ—¶é—´åºåˆ—æ•°æ®)"""
    # ç”Ÿæˆéšæœºçš„æ—¶é—´åºåˆ—æ•°æ®
    np.random.seed(42)  # ä¸ºäº†ç»“æœå¯é‡ç°
    data = np.random.randn(20, 14).astype(np.float32)
    
    # æ·»åŠ ä¸€äº›æ¨¡å¼ä½¿å…¶æ›´åƒçœŸå®çš„æ—¶é—´åºåˆ—æ•°æ®
    for i in range(14):
        data[:, i] += np.sin(np.linspace(0, 2*np.pi, 20)) * (i + 1) * 0.1
    
    return data


def main():
    parser = argparse.ArgumentParser(description="Triton Times_Classify å®¢æˆ·ç«¯")
    parser.add_argument("--url", default="localhost:8000", help="Triton æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--protocol", default="http", choices=["http", "grpc"], help="åè®®ç±»å‹")
    parser.add_argument("--model", default="Times_Classify", help="æ¨¡å‹åç§°")
    parser.add_argument("--data-file", help="è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„ (numpy .npy æ–‡ä»¶)")
    parser.add_argument("--batch-size", type=int, default=1, help="æ‰¹å¤„ç†å¤§å°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    print(f"ğŸš€ è¿æ¥åˆ° Triton æœåŠ¡å™¨: {args.url} (åè®®: {args.protocol})")
    client = TritonClient(url=args.url, protocol=args.protocol)
    
    # æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
    if not client.check_server_health():
        return
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    client.list_models()
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_metadata, model_config = client.get_model_info(args.model)
    if model_metadata is None:
        return
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    if args.data_file:
        try:
            data = np.load(args.data_file)
            print(f"ğŸ“ ä»æ–‡ä»¶åŠ è½½æ•°æ®: {args.data_file}")
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}")
            return
    else:
        print("ğŸ² ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
        data = generate_sample_data()
    
    print(f"ğŸ“Š è¾“å…¥æ•°æ®å½¢çŠ¶: {data.shape}")
    print(f"ğŸ“Š æ•°æ®èŒƒå›´: [{data.min():.4f}, {data.max():.4f}]")
    
    # æ‰§è¡Œé¢„æµ‹
    print(f"\nğŸ”® å¼€å§‹æ¨ç†...")
    result = client.predict_with_labels(args.model, data)
    
    if result:
        print(f"\nâœ… æ¨ç†å®Œæˆ!")
        print(f"ğŸ“ˆ ç»“æœæ‘˜è¦:")
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        print("âŒ æ¨ç†å¤±è´¥")


if __name__ == "__main__":
    main()
