version: '3.8'

services:
  tritonserver:
    image: nvcr.io/nvidia/tritonserver:25.07-py3-igpu-sdk
    runtime: nvidia
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    volumes:
      - ./:/workspace/triton-inference-server
    # command: tritonserver --model-repository=/workspace/triton-inference-server/model_repository
    stdin_open: true  # 保持标准输入打开
    tty: true         # 启用伪终端
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]